{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64457169-0b6e-411e-a1da-02c8a7d4ac8d",
   "metadata": {},
   "source": [
    "# Bedrock Knowledge Base Retrieval and Generation with Reranking\n",
    "\n",
    "The Rerank API in Amazon Bedrock is a new feature that improves the accuracy and relevance of responses in Retrieval-Augmented Generation (RAG) applications. It supports reranker models that rank a set of retrieved documents based on their relevance to a user's query, helping to prioritize the most relevant content for response generation.\n",
    "\n",
    "## Key features and use cases:\n",
    "\n",
    "1. **Enhancing RAG applications**: The Rerank API addresses challenges in semantic search, particularly with complex or ambiguous queries. For example, it can help a customer service chatbot focus on return policies rather than shipping guidelines when asked about returning an online purchase.\n",
    "\n",
    "2. **Improving search relevance**: It enables developers to significantly enhance their search relevance and content ranking capabilities, making enterprise-grade search technology more accessible.\n",
    "\n",
    "3. **Optimizing context window usage**: By ensuring the most useful information is sent to the foundation model, it potentially reduces costs and improves response accuracy.\n",
    "\n",
    "4. **Flexible integration**: The Rerank API can be used independently to rerank documents even if you're not using Amazon Bedrock Knowledge Bases.\n",
    "\n",
    "5. **Multiple model support**: At launch, it supports Amazon Rerank 1.0 and Cohere Rerank 3.5 models.\n",
    "\n",
    "6. **Customizable configurations**: Developers can specify additional model configurations as key-value pairs for more tailored reranking.\n",
    "\n",
    "The Rerank API is available in select AWS Regions, including US West (Oregon), Canada (Central), Europe (Frankfurt), and Asia Pacific (Tokyo). It can be integrated into existing systems at scale, whether keyword-based or semantic, through a single API call in Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3958d",
   "metadata": {},
   "source": [
    "![Reranking](./reranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-section",
   "metadata": {},
   "source": [
    "## 1: Import Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions from advanced_rag_utils\n",
    "from advanced_rag_utils import (\n",
    "    load_variables,\n",
    "    setup_bedrock_client,\n",
    "    get_value_by_key_path,\n",
    "    invoke_bedrock_converse,\n",
    "    search_kb_simple,\n",
    "    rerank_results,\n",
    "    search_rerank_combine\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ecae6-7c0a-4490-857c-c41e5c8d445e",
   "metadata": {},
   "source": [
    "## 2: Load Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a1a59-a87c-4953-9b79-1194f7fcce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration variables\n",
    "variables = load_variables(\"../variables.json\")\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c78a0f-b980-4753-b2e0-5dcd0b0f44cb",
   "metadata": {},
   "source": [
    "## 3: Define ARN and Configuration Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85280494-5d51-4c3b-939c-b4f482e422c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up configuration for Bedrock\n",
    "region_name = variables['regionName'] \n",
    "account_number = variables['accountNumber']\n",
    "knowledge_base_id = variables['kbSemanticChunk']\n",
    "model_id = 'us.amazon.nova-pro-v1:0' \n",
    "\n",
    "# Define ARNs (Amazon Resource Names) for the model\n",
    "model_arn = f\"arn:aws:bedrock:us-west-2:{account_number}:inference-profile/{model_id}\"\n",
    "rerank_model_arn = \"arn:aws:bedrock:us-west-2::foundation-model/cohere.rerank-v3-5:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b99bb-d6c8-4283-bdac-51919d025d45",
   "metadata": {},
   "source": [
    "## 4: Initialize Bedrock Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80f441-1cd3-4ea0-8d04-5043c2a8ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock clients\n",
    "bedrock_agent_client = setup_bedrock_client(region_name)\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e95426-0f43-4f3e-a49f-13c756c163d6",
   "metadata": {},
   "source": [
    "## 5: Get Initial Results from KB (Without Reranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692b96c-810a-40b0-9e1e-8ccf6b90a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"What is red teaming? How can it be used with text to SQL?\"\n",
    "\n",
    "# Search the knowledge base without reranking\n",
    "number_of_results = 5\n",
    "original_kb_results = search_kb_simple(\n",
    "    query=query,\n",
    "    knowledge_base_id=knowledge_base_id,\n",
    "    bedrock_client=bedrock_agent_client,\n",
    "    num_results=number_of_results,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Combine results into a context string\n",
    "kb_context = '\\n\\n'.join(original_kb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a0bfd-7322-47a0-8af9-7904acb3dee6",
   "metadata": {},
   "source": [
    "## 6: Get Response from LLM (Without Reranking)\n",
    "We will use the results we receive from Knowledge Base (KB) as-is. We are not doing any reranking yet.\n",
    "We will send the context from KB and the user query to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288e51c-0f49-4c82-ae74-0d61ec9527fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the system prompt\n",
    "system_prompt = f\"\"\"\n",
    "Please use the context below to respond to the question. \n",
    "If you have enough information to answer the question, please explain the reasoning behind the response.\n",
    "If you do not have enough information to answer the question, please don't guess. Instead, just say I don't know with the reason.\n",
    "CONTEXT:\n",
    "{kb_context}\n",
    "\"\"\"\n",
    "\n",
    "# Get response from the LLM\n",
    "answer, response = invoke_bedrock_converse(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=query,\n",
    "    model_id=model_id,\n",
    "    bedrock_client=bedrock_runtime_client,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "print(\"Response without reranking:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59048571-f4f3-4667-95be-82c77bb98242",
   "metadata": {},
   "source": [
    "## 7: Get More Results and Apply Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b09d8-8ce9-4664-822e-ed5b67c5e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more results initially, then we'll rerank them\n",
    "number_of_results = 20\n",
    "original_kb_results = search_kb_simple(\n",
    "    query=query,\n",
    "    knowledge_base_id=knowledge_base_id,\n",
    "    bedrock_client=bedrock_agent_client,\n",
    "    num_results=number_of_results,\n",
    "    region_name=region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058dede6-07cf-4fd9-b604-9caab89a3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank the results\n",
    "reranked_result_count = 5\n",
    "reranked_json = rerank_results(\n",
    "    query=query,\n",
    "    documents=original_kb_results,\n",
    "    rerank_model_arn=rerank_model_arn,\n",
    "    bedrock_client=bedrock_agent_client,\n",
    "    reranked_result_count=reranked_result_count,\n",
    "    region_name=region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c500b-5ba9-46b6-9cff-d7d51f574fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine reranked results into a context string\n",
    "kb_context = \"\"\n",
    "for result in reranked_json['reranked_results']:\n",
    "    kb_context += result['text'] + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-response-section",
   "metadata": {},
   "source": [
    "## 8: Get Response from LLM (With Reranking)\n",
    "Now we'll send the reranked context to the LLM and see how the response improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9979067-1138-4f75-b745-f53e629ce89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the system prompt with reranked context\n",
    "system_prompt = f\"\"\"\n",
    "Please use the context below to respond to the question. \n",
    "If you have enough information to answer the question, please explain the reasoning behind the response.\n",
    "If you do not have enough information to answer the question, please don't guess. Instead, just say I don't know with the reason.\n",
    "CONTEXT:\n",
    "{kb_context}\n",
    "\"\"\"\n",
    "\n",
    "# Get response from the LLM with reranked context\n",
    "answer, result = invoke_bedrock_converse(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=query,\n",
    "    model_id=model_id,\n",
    "    bedrock_client=bedrock_runtime_client,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "print(\"Response with reranking:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "response-metadata-section",
   "metadata": {},
   "source": [
    "## 9: View Response Metadata (Tokens, Latency, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161508b1-2db4-483a-bb5d-863a6347d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed response metadata\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "all-in-one-section",
   "metadata": {},
   "source": [
    "## 10: Alternative Approach - All-in-One Function\n",
    "This alternative approach uses a single function to handle search, reranking, and combining results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "all-in-one-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the comprehensive function to search, rerank, and combine\n",
    "query = \"What is red teaming? How can it be used with text to SQL?\"\n",
    "\n",
    "# Get combined context and reranking details\n",
    "combined_context, reranked_details = search_rerank_combine(\n",
    "    query=query,\n",
    "    knowledge_base_id=knowledge_base_id,\n",
    "    rerank_model_arn=rerank_model_arn,\n",
    "    bedrock_client=bedrock_agent_client,\n",
    "    initial_result_count=20,\n",
    "    reranked_result_count=5,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Create the system prompt\n",
    "system_prompt = f\"\"\"\n",
    "Please use the context below to respond to the question. \n",
    "If you have enough information to answer the question, please explain the reasoning behind the response.\n",
    "If you do not have enough information to answer the question, please don't guess. Instead, just say I don't know with the reason.\n",
    "CONTEXT:\n",
    "{combined_context}\n",
    "\"\"\"\n",
    "\n",
    "# Get response from the LLM\n",
    "answer, result = invoke_bedrock_converse(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=query,\n",
    "    model_id=model_id,\n",
    "    bedrock_client=bedrock_runtime_client,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "print(\"Response using all-in-one function:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
