{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c01279f-f6b3-46a2-9c76-ecf34f9dc275",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with Semantic Chunking Strategy\n",
    "#### What will we do in this workshop?\n",
    "1. Create a Knowledgebase (KB) in the vector database.\n",
    "2. We will create a data source for the KB. The data source will be the Amazon Science and 10K documents stored in S3.\n",
    "3. We will ingest the data from S3, use Semantic Chunking to chunk the data, generate vector embeddings, and store the chunks and their corresponding vector embeddings in the KB.\n",
    "4. We will then ask some questions and query the KB to return some chunks and inspect relevancy score.\n",
    "<br>Note: We are not sending the query and its chunks to a LLM in this notebook. We will do that in other notebooks.\n",
    "![We are generating vector embeddings and storing them in a KB in Vector Database](./Semantic_Chunking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0a202-87f1-4a0f-9d2f-e697e6b98bee",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "Semantic chunking analyzes the relationships within a text (using vector embeddings) and creates chunks based on the semantic similarity calculated by the embedding model. Please note that this will result in chunks with varying sizes. This approach preserves the information’s integrity during retrieval, helping to ensure accurate and contextually appropriate results. <br>\n",
    "<br>![How Semantic Chunking Works](./Semantic_how_it_works.png)\n",
    "\n",
    "## Benefits\n",
    "\n",
    "* By focusing on the text’s meaning and context, semantic chunking significantly improves the quality of retrieval. It should be used in scenarios where maintaining the semantic integrity of the text is crucial.\n",
    "\n",
    "* Although this method is more computationally intensive than fixed-size chunking, it can be beneficial for chunking documents where contextual boundaries aren’t clear—for example, legal documents, technical manuals, documents with too many tables.\n",
    "\n",
    "## Cost Considerations\n",
    "\n",
    "* Since Semantic chunking process generates vector embeddings to find chunk boundaries, this will result in increase API calls to an embedding LLM. Thus, expect relatively higher costs than fixed size chunking.\n",
    "* However, the bulk of these operations happen the first time the documents are processed. In a steady state situation these costs will be incurred only for new documents or changes documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67e01c-6f6a-4b11-bb8f-256fb15ef2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a module with few helper functions. \n",
    "# These functions will help us create knowledge base (KB), create data source for KB, and ingest data using semantic chunking to KB.\n",
    "\n",
    "import importlib\n",
    "import advanced_rag_utils\n",
    "\n",
    "# Reload module\n",
    "importlib.reload(advanced_rag_utils)\n",
    "\n",
    "# Re-import all functions\n",
    "from advanced_rag_utils import *\n",
    "\n",
    "from datetime import datetime, timedelta, UTC\n",
    "\n",
    "notebook_start_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b874664-bd21-4929-8fe0-165da697e11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's load the variables we saved in the first notebook. We will use these variables\n",
    "import json\n",
    "with open(\"../variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc4268-af67-46e1-8237-f818583a868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe related to costs from a csv file (if it already exists)\n",
    "df_costs = load_df_from_csv()\n",
    "df_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805c37f-2e62-4945-b0e3-05af7db0e60d",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base\n",
    "Let's specify  chunking strategy, name and descripotion for Knowledge Base (KB) and create a KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ac212-083b-4bd0-abf1-e73919a054f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "kb_chunking_strategy = \"semantic\" # [\"fixed\", \"hierarchical\", \"semantic\", \"custom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c4796-f812-49c1-8188-962cadd6eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_name = f\"advanced-rag-workshop-{kb_chunking_strategy}-chunking\"\n",
    "\n",
    "kb_description = \"Knowledge base using Amazon OpenSearch Service as a vector store\"\n",
    "\n",
    "kb = create_kb(kb_name, kb_description, kb_chunking_strategy, variables, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfebc0-bdeb-4f43-a0a0-921d34be6aba",
   "metadata": {},
   "source": [
    "### 2. Create Datasource for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc990e-b0da-4853-a48a-071219d1461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name = f\"advanced-rag-example-{kb_chunking_strategy}\"\n",
    "\n",
    "ds_object = create_data_source_for_kb(kb_chunking_strategy, data_source_name, kb, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d415f-d61f-49ae-9d9a-1ea3063a1452",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cc69d",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in a Knowledge Base (KB) in OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e084b1-9c83-45b2-a08f-36b8d2bf0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "ingestion_start_time = datetime.now(UTC)\n",
    "create_ingestion_job(kb, ds_object, variables)\n",
    "ingestion_end_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a5d31-22fa-4a23-be51-b479d02da181",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken = (ingestion_end_time-ingestion_start_time).total_seconds()\n",
    "print(f\"time taken to ingest into KB = {fmt_n(time_taken)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db1388-55e0-4db9-a05d-b7c2f2cf5379",
   "metadata": {},
   "source": [
    "## Embedding LLM Costs\n",
    "1. Specify model id\n",
    "2. Specify start and end time\n",
    "3. Invoke a helper function to query cloud watch\n",
    "5. Calculate costs (please note that pricing is subject to change per region and over time)\n",
    "\n",
    "<br>![Embedding LLM Input Token Costs](./Input_token_embedding_llm_costs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc155e-d7c2-4979-a09d-997af630e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_embedding_cost = get_bedrock_token_based_cost(model_id, ingestion_start_time, ingestion_end_time)\n",
    "print(json.dumps(vector_store_embedding_cost, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1865a0-f52e-4348-8119-61676c18ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add or update the cost binfo to dataframe. \n",
    "# This will help us compare the costs from various chunking strategies visually.\n",
    "new_row = {\n",
    "    'chunking_algo': kb_chunking_strategy,\n",
    "    'embedding_seconds': vector_store_embedding_cost['duration in minutes']*60,\n",
    "    'input_tokens': vector_store_embedding_cost['input_tokens'],\n",
    "    'invocation_count': vector_store_embedding_cost['invocation_count'],\n",
    "    'total_token_costs': vector_store_embedding_cost['total token costs']\n",
    "}\n",
    "df_costs = update_or_add_row(df_costs, new_row)\n",
    "df_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1d336-ff62-4844-991c-46cde76a00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the df\n",
    "save_df_to_csv(df_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988051f4-2741-4180-af71-d445f2eaf37d",
   "metadata": {},
   "source": [
    "### 4. Retrieve: Use input query to RETRIEVE chunks from Vector Database\n",
    "We will use a helper function where you can specify the number of chunks to extract.<br>\n",
    "The helper function will 1/ generate a vector embedding for the query, 2/ search the vector embedding in the Knowledge Base (KB) vector database, 3/ get the number of chunks specified, 4/ Optionally, you can also specify minimum score for similarity in which case the helper function will get chunks with at least the minimum relevancy.\n",
    "\n",
    "<b>Warning: After data is ingested into a KB, when you query immediately, the results might be empty because of eventual consistency. If that happens, please wait for a few seconds and then retry.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f992ba-d37d-4c3f-89d3-128f3a8920ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's pick the chunks with some minimum relevance score for the same question.\n",
    "query = \"Who is the CEO, CFO, and CTO of Amazon?\"\n",
    "\n",
    "#specify the number of chunks\n",
    "n_chunks = 5\n",
    "\n",
    "#Let's specify a minimum similarity score. We should see less chunks retrieved as compared to the previous invocation.\n",
    "min_score = 0.30\n",
    "\n",
    "# get chunks from KB\n",
    "chunks_from_kb = retrieve_from_kb(query, kb, n_chunks, variables, min_score)\n",
    "\n",
    "print(json.dumps(chunks_from_kb, indent=2))\n",
    "\n",
    "# You should see less number of chunks retrieved as compared to the previous cell \n",
    "# because of the minimum relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b2372-8932-4df2-8c73-b5960655752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's summarize with total chunks, minimum score, maximum score, average score, \n",
    "# and lastly the number of chunks with a score more than a specified threshold.\n",
    "score_threshold = 0.30\n",
    "score_structure = analyze_chunk_scores_above_threshold(chunks_from_kb, score_threshold)\n",
    "print(json.dumps(score_structure, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6c818-e550-4c65-bbca-278c58fab5f4",
   "metadata": {},
   "source": [
    "### Cost Summary for Running This Notebook\n",
    "In this notebook, we have used an embedding LLM for two purposes. \n",
    "1. Populate a vector store for six PDF files and one CSV file. (7 documents in total)\n",
    "2. Generate a query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84a126-5ee7-42b7-b891-100010e2d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Marking notebook endtime\n",
    "notebook_end_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434aba2-ccef-4531-a50d-9734777d1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from advanced_rag_utils import embedding_cost_report\n",
    "\n",
    "cost_for_notebook = get_bedrock_token_based_cost(model_id, notebook_start_time, notebook_end_time)\n",
    "\n",
    "# Your assumptions for your use case:\n",
    "scenario_number_of_documents = 100000\n",
    "scenario_number_of_queries =   5000000\n",
    " \n",
    "display(Markdown(embedding_cost_report(vector_store_embedding_cost, cost_for_notebook, scenario_number_of_documents, scenario_number_of_queries)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
